{"name":"Machine learning project","tagline":"","body":"---\r\ntitle: \"Practical Machine Learning Project\"\r\nauthor: \"F. Novak\"\r\ndate: \"May 13, 2015\"\r\noutput: \r\n  html_document:\r\n    number_sections: true\r\n---\r\n\r\n# Introduction\r\n## Summary\r\nA model was constructed and trained to use human activity data to predict the manner in which weight lifing excercise was performed, either correctly or following one of four common errors.  Excellent results, although computationally expensive, were obtained with a Random Forest algorithm.  Respectable results, in addition to being computationally inexpensive, were obtained with Linear Discriminant Analysis.\r\n\r\n## Purpose\r\nThe goal of the project is to use motion data to predict the manner in which they did the exercise, whether correctly (A) or following one of four common errors (B - E) \r\n\r\n## Source of Data\r\nHere is the citation for the data used for this project.\r\n\r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\r\n\r\nThe training and testing data were downloaded on 5/20/2015 at 1154AM NY time from the following links:\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pmltraining.csv\r\n\r\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pmltesting.csv\r\n\r\n# Data Preprocessing & Slicing\r\n## Steps to Tidy Up Data\r\nThe following columns were removed from the data.\r\n\r\n* The first 7 columns because they were administrative. One column was just an index, and the others were related to the date, which would not be useful for predicting future results. \r\n* The columns that were mostly NA.  It was found that most columns had no NA data, but some had~ 97% NA and were removed.  \r\n* The columns containing summary stastics (kurtosis, skewness, min, max, and amp), and were removed.\r\n\r\nThe code below preforms this pre-processing.\r\n```{r, eval = FALSE}\r\nd <- read.csv(\"pml-training.csv\")\r\n\r\nd1 <- d1[,8:ncol(d1)]  # first bullet above\r\nd <- d[,colSums(is.na(d)) ==0]  # second bullet above\r\n# determine rows with summary statistics for third bullet\r\nkurt <- grep(\"^kurt\",names(d))\r\nskew <- grep(\"^skew\",names(d))\r\nmin <- grep(\"^min\",names(d))\r\nmax <- grep(\"^max\",names(d))\r\namp <- grep(\"^amp\",names(d))\r\nset = c(kurt,skew,min,max,amp)\r\nd1 <- select(d,-set)   ## third bullet above\r\n```\r\n\r\nThis left the dataset with all 19,622 rows, 53 columns.\r\n\r\n## Partitioning of Data\r\nThe data set was partitioned into a training set and a cross-validation set.  Of the total, 60% was used for training set and 40% for cross-validation.  This was somewhat arbitrary, but in line with convention.\r\n\r\n```{r, eval=FALSE}\r\ninTrain = createDataPartition(y=d1$classe, p = 0.6, list = FALSE)\r\ndt = d1[ inTrain,]\r\ndcv = d1[-inTrain,]\r\n```\r\n\r\n# Models\r\n## Expected Sample Error\r\nWith 19,622 rows and 53 columns of data, the main concern was about variance instead of bias.  The undesirable symptom is overfitting.  It was found that the training and cross-validation  errors were similar, so that overfitting is not a problem.\r\n\r\n## Description of Models\r\nThe results of three models are presented.  \r\n\r\n### Random Forest\r\nA model was trained with Random Forest algorithm in the caret package, with rather impressive results as tabulated in a later section.\r\n\r\n```{r, eval = FALSE}\r\nfit_rf <- train(classe ~ ., method=\"rf\", data=dt)\r\nyhat.rf.tr <- predict(fit_rf, newdata = dt)\r\nacc_rf.tr <- mean(yhat.rf.tr == dt$classe)\r\n```\r\n\r\nThe downside of the Random Forest algorithm was that it took several hours to run on a laptop computer. However, if very accurate results are desirable, this is a reasonable price to pay because it predicts results based on new data quickly.\r\n\r\n### Linear Discriminant Analysis\r\nA model was trained with Linear Discriminant algorithm in the caret package, with respectable results as tabulated in a later section.\r\n\r\n```{r, eval = FALSE}\r\nfit_lda <- train(classe ~ ., method=\"lda\", data=dt)\r\nyhat.lda.tr <- predict(fit_lda, newdata = dt)\r\nacc_lda.tr <- mean(yhat.lda.tr == dt$classe)\r\n```\r\n\r\nThe Linear Discriminant Algorithm ran quickly, within a few seconds.  This would be a strong advantage over Random Forest if it is important to train the model frequently.  However, the ease of training comes at the cost of less accuracy.\r\n\r\n### Trees\r\nA model was trained with Rpart (CART) algorithm in the caret package, but with rather unimpressive results.\r\n\r\n```{r, eval = FALSE}\r\nfit_cart <- train(classe ~ ., method=\"rpart\", data=dt)\r\nyhat.cart.tr <- predict(fit_cart, newdata = dt)\r\nacc_cart.tr <- mean(yhat.cart.tr == dt$classe)\r\n```\r\n\r\nThe trees (rpart or CART) algorithm ran fairly quickly, about the same as the linear discriminant analysis, but the accuracy was not as good.  The advantage of this algorithm is that the results are highly interpretable in that a figure can be drawn to show the sorting algorithm.  It is not shown in this report, however, because the results are not very accurate.\r\n\r\n\r\n## Cross Validation\r\nEach of the three models was cross-validated using the data that was held back. The code is immediately below, and the results are tabulated in the next section.\r\n\r\n```{r, eval=FALSE}\r\n# LDA (cross-validate)\r\nyhat.lda.cv <- predict(fit_lda, newdata = dcv)\r\nacc_lda.cv <- mean(yhat.lda.cv == dcv$classe)\r\n\r\n# Random Forest (cross-validate)\r\nyhat.rf.cv <- predict(fit_rf, newdata = dcv)\r\nacc_rf.cv <- mean(yhat.rf.cv == dcv$classe)\r\n\r\n# Trees (cross-validate)\r\nyhat.cart.cv <- predict(fit_cart, newdata = dcv)\r\nacc_cart.cv <- mean(yhat.cart.cv == dcv$classe)\r\n```\r\n\r\n# Results\r\n## Numerical Results\r\nModel Type | Training Accuracy | Cross Validation Accuracy| Comments\r\n------------- | ---------------|------------------|----\r\nRandom Forest | 100% | 99.0% | Very accurate, but took hours to run\r\nLinear Discriminant Analysis | 70.1% | 70.7%| Respectable results; ran quickly\r\nTrees (rpart) | 49.6% | 49.2% | Ran quickly but worst results\r\n\r\nThe Random Forest model was used to submit the predictions of test cases, and obtained a perfect score (20/20) on the first try for each submission.\r\n\r\n## Discussion of Sample Error\r\nClearly the cross-validation error was not far worse than the training error.  In fact, in the case of linear discriminant analysis, it was better.  When the test cases were submitted using the randome forest model, the cases were predicted with 100% accuracy (20/20) on the first try.  This leads to the conclusion that neither bias nor overfitting was a problem.  \r\n\r\n# Conclusions\r\nExcellent results, but computationally expensive, were obtained with a Random Forest algorithm.  Respectable results, and computationally inexpensive, were obtained with Linear Discriminant Analysis. Depending on the application, either of these models would suffice.  The Random Forest algorithm was used for submission of results.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}