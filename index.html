<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Machine learning project by fgnovak</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Machine learning project</h1>
        <p></p>

        <p class="view"><a href="https://github.com/fgnovak/Machine_learning_project">View the Project on GitHub <small>fgnovak/Machine_learning_project</small></a></p>


        <ul>
          <li><a href="https://github.com/fgnovak/Machine_learning_project/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/fgnovak/Machine_learning_project/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/fgnovak/Machine_learning_project">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <hr>

<p>title: "Practical Machine Learning Project"
author: "F. Novak"
date: "May 13, 2015"
output: 
  html_document:</p>

<h2>
<a id="----number_sections-true" class="anchor" href="#----number_sections-true" aria-hidden="true"><span class="octicon octicon-link"></span></a>    number_sections: true</h2>

<h1>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h1>

<h2>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h2>

<p>A model was constructed and trained to use human activity data to predict the manner in which weight lifing excercise was performed, either correctly or following one of four common errors.  Excellent results, although computationally expensive, were obtained with a Random Forest algorithm.  Respectable results, in addition to being computationally inexpensive, were obtained with Linear Discriminant Analysis.</p>

<h2>
<a id="purpose" class="anchor" href="#purpose" aria-hidden="true"><span class="octicon octicon-link"></span></a>Purpose</h2>

<p>The goal of the project is to use motion data to predict the manner in which they did the exercise, whether correctly (A) or following one of four common errors (B - E) </p>

<h2>
<a id="source-of-data" class="anchor" href="#source-of-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Source of Data</h2>

<p>Here is the citation for the data used for this project.</p>

<p>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>

<p>The training and testing data were downloaded on 5/20/2015 at 1154AM NY time from the following links:
<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pmltraining.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pmltraining.csv</a></p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pmltesting.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pmltesting.csv</a></p>

<h1>
<a id="data-preprocessing--slicing" class="anchor" href="#data-preprocessing--slicing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Preprocessing &amp; Slicing</h1>

<h2>
<a id="steps-to-tidy-up-data" class="anchor" href="#steps-to-tidy-up-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Steps to Tidy Up Data</h2>

<p>The following columns were removed from the data.</p>

<ul>
<li>The first 7 columns because they were administrative. One column was just an index, and the others were related to the date, which would not be useful for predicting future results. </li>
<li>The columns that were mostly NA.  It was found that most columns had no NA data, but some had~ 97% NA and were removed.<br>
</li>
<li>The columns containing summary stastics (kurtosis, skewness, min, max, and amp), and were removed.</li>
</ul>

<p>The code below preforms this pre-processing.</p>

<pre lang="r,"><code>d &lt;- read.csv("pml-training.csv")

d1 &lt;- d1[,8:ncol(d1)]  # first bullet above
d &lt;- d[,colSums(is.na(d)) ==0]  # second bullet above
# determine rows with summary statistics for third bullet
kurt &lt;- grep("^kurt",names(d))
skew &lt;- grep("^skew",names(d))
min &lt;- grep("^min",names(d))
max &lt;- grep("^max",names(d))
amp &lt;- grep("^amp",names(d))
set = c(kurt,skew,min,max,amp)
d1 &lt;- select(d,-set)   ## third bullet above
</code></pre>

<p>This left the dataset with all 19,622 rows, 53 columns.</p>

<h2>
<a id="partitioning-of-data" class="anchor" href="#partitioning-of-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Partitioning of Data</h2>

<p>The data set was partitioned into a training set and a cross-validation set.  Of the total, 60% was used for training set and 40% for cross-validation.  This was somewhat arbitrary, but in line with convention.</p>

<pre lang="r,"><code>inTrain = createDataPartition(y=d1$classe, p = 0.6, list = FALSE)
dt = d1[ inTrain,]
dcv = d1[-inTrain,]
</code></pre>

<h1>
<a id="models" class="anchor" href="#models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Models</h1>

<h2>
<a id="expected-sample-error" class="anchor" href="#expected-sample-error" aria-hidden="true"><span class="octicon octicon-link"></span></a>Expected Sample Error</h2>

<p>With 19,622 rows and 53 columns of data, the main concern was about variance instead of bias.  The undesirable symptom is overfitting.  It was found that the training and cross-validation  errors were similar, so that overfitting is not a problem.</p>

<h2>
<a id="description-of-models" class="anchor" href="#description-of-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Description of Models</h2>

<p>The results of three models are presented.  </p>

<h3>
<a id="random-forest" class="anchor" href="#random-forest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Forest</h3>

<p>A model was trained with Random Forest algorithm in the caret package, with rather impressive results as tabulated in a later section.</p>

<pre lang="r,"><code>fit_rf &lt;- train(classe ~ ., method="rf", data=dt)
yhat.rf.tr &lt;- predict(fit_rf, newdata = dt)
acc_rf.tr &lt;- mean(yhat.rf.tr == dt$classe)
</code></pre>

<p>The downside of the Random Forest algorithm was that it took several hours to run on a laptop computer. However, if very accurate results are desirable, this is a reasonable price to pay because it predicts results based on new data quickly.</p>

<h3>
<a id="linear-discriminant-analysis" class="anchor" href="#linear-discriminant-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Discriminant Analysis</h3>

<p>A model was trained with Linear Discriminant algorithm in the caret package, with respectable results as tabulated in a later section.</p>

<pre lang="r,"><code>fit_lda &lt;- train(classe ~ ., method="lda", data=dt)
yhat.lda.tr &lt;- predict(fit_lda, newdata = dt)
acc_lda.tr &lt;- mean(yhat.lda.tr == dt$classe)
</code></pre>

<p>The Linear Discriminant Algorithm ran quickly, within a few seconds.  This would be a strong advantage over Random Forest if it is important to train the model frequently.  However, the ease of training comes at the cost of less accuracy.</p>

<h3>
<a id="trees" class="anchor" href="#trees" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trees</h3>

<p>A model was trained with Rpart (CART) algorithm in the caret package, but with rather unimpressive results.</p>

<pre lang="r,"><code>fit_cart &lt;- train(classe ~ ., method="rpart", data=dt)
yhat.cart.tr &lt;- predict(fit_cart, newdata = dt)
acc_cart.tr &lt;- mean(yhat.cart.tr == dt$classe)
</code></pre>

<p>The trees (rpart or CART) algorithm ran fairly quickly, about the same as the linear discriminant analysis, but the accuracy was not as good.  The advantage of this algorithm is that the results are highly interpretable in that a figure can be drawn to show the sorting algorithm.  It is not shown in this report, however, because the results are not very accurate.</p>

<h2>
<a id="cross-validation" class="anchor" href="#cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross Validation</h2>

<p>Each of the three models was cross-validated using the data that was held back. The code is immediately below, and the results are tabulated in the next section.</p>

<pre lang="r,"><code># LDA (cross-validate)
yhat.lda.cv &lt;- predict(fit_lda, newdata = dcv)
acc_lda.cv &lt;- mean(yhat.lda.cv == dcv$classe)

# Random Forest (cross-validate)
yhat.rf.cv &lt;- predict(fit_rf, newdata = dcv)
acc_rf.cv &lt;- mean(yhat.rf.cv == dcv$classe)

# Trees (cross-validate)
yhat.cart.cv &lt;- predict(fit_cart, newdata = dcv)
acc_cart.cv &lt;- mean(yhat.cart.cv == dcv$classe)
</code></pre>

<h1>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h1>

<h2>
<a id="numerical-results" class="anchor" href="#numerical-results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Numerical Results</h2>

<table>
<thead>
<tr>
<th>Model Type</th>
<th>Training Accuracy</th>
<th>Cross Validation Accuracy</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Forest</td>
<td>100%</td>
<td>99.0%</td>
<td>Very accurate, but took hours to run</td>
</tr>
<tr>
<td>Linear Discriminant Analysis</td>
<td>70.1%</td>
<td>70.7%</td>
<td>Respectable results; ran quickly</td>
</tr>
<tr>
<td>Trees (rpart)</td>
<td>49.6%</td>
<td>49.2%</td>
<td>Ran quickly but worst results</td>
</tr>
</tbody>
</table>

<p>The Random Forest model was used to submit the predictions of test cases, and obtained a perfect score (20/20) on the first try for each submission.</p>

<h2>
<a id="discussion-of-sample-error" class="anchor" href="#discussion-of-sample-error" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discussion of Sample Error</h2>

<p>Clearly the cross-validation error was not far worse than the training error.  In fact, in the case of linear discriminant analysis, it was better.  When the test cases were submitted using the randome forest model, the cases were predicted with 100% accuracy (20/20) on the first try.  This leads to the conclusion that neither bias nor overfitting was a problem.  </p>

<h1>
<a id="conclusions" class="anchor" href="#conclusions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusions</h1>

<p>Excellent results, but computationally expensive, were obtained with a Random Forest algorithm.  Respectable results, and computationally inexpensive, were obtained with Linear Discriminant Analysis. Depending on the application, either of these models would suffice.  The Random Forest algorithm was used for submission of results.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/fgnovak">fgnovak</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
